* Decoding Surface Touch Typing from Hand-Tracking
  * Abstract
    * We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efﬁciently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contactbased text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an ofﬂine study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments.

* Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion
  * Abstract
    * We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users’ intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user’s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1%. This performance represents a 19.6% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.

* ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data
  * Abstract
    * Ten-finger freehand mid-air typing is a potential solution for post-desktop interaction. However, the absence of tactile feedback as well as the inability to accurately distinguish tapping ﬁnger or target keys exists as the major challenge for mid-air typing. In this paper, we present ATK, a novel interaction technique that enables freehand ten-ﬁnger typing in the air based on 3D hand tracking data. Our hypothesis is that expert typists are able to transfer their typing ability from physical keyboards to mid-air typing. We followed an iterative approach in designing ATK. We ﬁrst empirically investigated users’ mid-air typing behavior, and examined ﬁngertip kinematics during tapping, correlated movement among ﬁngers and 3D distribution of tapping endpoints. Based on the ﬁndings, we proposed a probabilistic tap detection algorithm, and augmented Goodman’s input correction model to account for the ambiguity in distinguishing tapping ﬁnger. We ﬁnally evaluated the performance of ATK with a 4-block study. Participants typed 23.0 WPM with an uncorrected word-level error rate of 0.3% in the ﬁrst block, and later achieved 29.2 WPM in the last block without sacriﬁcing accuracy.

* Selection-Based Mid-Air Text Entry on Large Displays
  * Abstract
    * Most text entry methods require users to have physical devices within reach. In many contexts of use, such as around large displays where users need to move freely, device-dependent methods are ill suited. We explore how selection-based text entry methods may be adapted for use in mid-air. Initially, we analyze the design space for text entry in mid-air, focusing on singlecharacter input with one hand. We propose three text entry methods: H4 MidAir (an adaptation of a game controller-based method by MacKenzie et al. [21]), MultiTap (a mid-air variant of a mobile phone text entry method), and Projected QWERTY (a mid-air variant of the QWERTY keyboard). After six sessions, participants reached an average of 13.2 words per minute (WPM) with the most successful method, Projected QWERTY. Users rated this method highest on satisfaction and it resulted in the least physical movement.

* Vision-Based Handwriting Recognition for Unrestricted Text Input in Mid-Air
  * Abstract
    * We propose a vision-based system that recognizes handwriting in mid-air. The system does not depend on sensors or markers attached to the users and allows unrestricted character and word input from any position. It is the result of combining handwriting recognition based on Hidden Markov Models with multi-camera 3D hand tracking. We evaluated the system for both quantitative and qualitative aspects. The system achieves recognition rates of 86.15% for character and 97.54% for small-vocabulary isolated word recognition. Limitations are due to slow and low-resolution cameras or physical strain. Overall, the proposed handwriting recognition system provides an easy-to-use and accurate text input modality without placing restrictions on the users.

* AirStroke: Bringing Unistroke Text Entry to Freehand Gesture Interfaces
  * Abstract
    * In this paper, we explore the opportunity of bringing unistroke text entry to freehand gesture interfaces. Using existing text entry methods directly in such interfaces is impractical because of the differences between freehand gestures and traditional forms of input. To address this problem, we consider the design constraints of text entry methods using freehand gestures, and present AirStroke, a new technique based on a reengineering of the well-known unistroke technique Graffiti. Using Graffiti’s alphabet, AirStroke takes advantage of the richer input capabilities of two-handed freehand gestures by providing combined mode selection and character entry with one hand, as well as word completion with the other hand. A longitudinal study suggests that AirStroke has competitive speed and accuracy to unistroke methods based on stylus input.

* ARKB: 3D vision-based Augmented Reality Keyboard
  * Abstract
    * In this paper, we propose a wearable 3D Augmented Reality Keyboard (ARKB) which enables a user to type text or control CG objects without using conventional interfaces, such as keyboard or mouse. The proposed ARKB exploits 3D depth information obtained through a stereo camera attached to an HMD. The ARKB consists of three modules: (i) 3D vision-based tracking, (ii) natural interaction with fingers, and (iii) audiovisual feedback on the 3D video see-through HMD. The proposed ARKB can be applied as an interface for typing in AR environment. The remaining challenges are study on tracking method to improve accuracy and newly designed virtual keyboard which is proper in representing the advantage of the interaction in 3D space.
