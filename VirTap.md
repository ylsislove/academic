# VirTap

* TapType: Ten-finger text entry on everyday surfaces via Bayesian inference

  * Abstract

    * Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces—without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters’ prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6% after 30 minutes oftraining. Expert typists thereby consistently achieved more than 25WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface.
  * Introduction

    * Physical keyboards have been the go-to option for typing large amounts of text, especially since their commoditization alongside personal computers. Such keyboards are designed to support fast and bimanual use, while the arrangement of keys allows our hands and fingers to unfold their dexterous capabilities across the full space [13, 17]. A welcome side effect of physical keyboards is their ability to support prolonged use, allowing the user’s arms to rest on a surface during continued interaction while providing passive haptic feedback. This property has allowed them to become a key factor in accomplishing productivity tasks [28].
    * With rising popularity ofwearable and mobile devices such as smartphones, new types of keyboards have had to compromise on many of these desirable properties in order to improve portability. Since touch-screen devices integrate input and output into the same surface for direct interaction, keyboards necessarily need to fit the available real estate and cannot stretch to full size any longer. As a result, today’s keyboards often appear shrunk [67] or sparse [29, 51], which affords text input in mobile situations on the go, albeit at the cost of reduced comfort, accuracy, and speed.
    * To compensate for the input error that comes with smaller layouts that accommodate few fingers, smartphones implement language models to aid in detecting intended keys [21, 38, 67]. Researchers have used language models in conjunction with input decoding to port keyboard entry to even smaller surfaces, such as watches [66], and fingertips [74, 75]. These model-based implementations have since been ported back to soft keyboards on tablets that approach full-size input [55], but have also been appropriated to create novel keyboard designs (e.g., One Line Keyboard [41] or Invisible Typing [57, 80]).
    * In this paper, we introduce TapType, a novel text entry system that supports opportunistic, mobile, and full-size touch typing on flat surfaces. Users simply wear a TapType sensor band on either wrist, place their hands down, and start typing. TapType registers taps through inertial sensors and wirelessly offloads events for processing to our backend that predicts entered characters. TapType’s sole requirement on wrist sensors makes it a suitable portable text entry method with the unobtrusive and socially accepted form factor of a fitness tracker. When typing with TapType, users can transfer their already practiced and internalized fast and eyes-free skills oftouch typing on a keyboard to any situation on the go. This makes TapType a promising text entry interface for a wide range of mobile, desktop, and spatial-computing use cases.
* TapID: Rapid Touch Interaction in Virtual Reality using Wearable Sensing

  * Abstract

    * Current Virtual Reality systems typically use cameras to capture user input from controllers or free-hand mid-air interaction. In this paper, we argue that this is a key impediment to productivity scenarios in VR, which require continued interaction over prolonged periods of time—a requirement that controller or free-hand input in mid-air does not satisfy. To address this challenge, we bring rapid touch interaction on surfaces to Virtual Reality—the input modality that users have grown used to on phones and tablets for continued use. We present TapID, a wrist-based inertial sensing system that complements headset-tracked hand poses to trigger input in VR. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the ﬁnger used for touch. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and hand-agnostic ﬁnger-identiﬁcation accuracy (within-user: F1 = 0.93; across users: F1 =0.91 after 10 reﬁnement taps and F1 =0.87 without reﬁnement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input and that are uniquely enabled by TapID, including UI control, rapid keyboard typing and piano playing, as well as surface gestures.
  * Introduction

    * The latest Mixed Reality systems incorporate hand pose recognition for input detection, for Augmented Reality (e.g., Hololens 2) and Virtual Reality (e.g., Quest 2) alike. While previous device generations had largely relied on hand-held controllers for input, the transition to controller-free, hand pose and gesture-operated input is apparent, both in the research communities (e.g., [47]) as well as the commercial domain (e.g., [11, 12, 33]).
    * In Augmented Reality, hand tracking is well-suited to accompany the work on physical objects, for example in maintenance and repair [21] or manufacturing and assembly [42]. Working on physical objects allows the user to hold onto something, thus preventing fatigue during use [22]. In contrast, for the interaction with objects in Virtual Reality, where content is intangible outside passive haptics systems (e.g., [9,24,30]), free-hand interaction can become a burden once it exceeds quick interactions [25].
    * In this paper, we propose moving all direct interaction in VR to passive surfaces. Compared to mid-air interaction, touch interaction on surfaces provides users with an opportunity to rest their arms between interactions while simultaneously offering physical support during prolonged interactions. At the same time, the surface provides haptic feedback alongside a frame of reference for proprioception, affording quick and precise interaction.
    * The core challenge of touch interaction in VR systems is the precise detection of touch events; because hands are tracked from the head-mounted systems themselves, this single vantage point can lead to inaccuracy in depth sensing and thus make touch/no-touch discrimination and contact locating [2] challenging. A variety of approaches have been proposed to detect touch on surfaces using depth cameras, for example for stationary [46] and headset-integrated systems [47]. To combat noise in-depth measurements, they integrate filters for reliable touch detection and rejection. Both, adding ﬁlters as well as the framerate of the used camera inherently limit the speed of interaction that systems can reliably detect.
    * We address the challenge of reliable and quick touch detection with our band TapID, which users wear around their wrist. Through built-in inertial sensors, TapID does not just detect touch events on passive surfaces but additionally identifies the ﬁnger used for touch. TapID thus complements the headset’s optical tracking of hands and ﬁngers, with which we combine TapID’s detected touches to trigger input events in VR. Our approach, therefore, allows applications and interaction modalities known from tablets and phones to seamlessly and reliably transfer to VR scenarios.
* TypingRing: A Wearable Ring Platform for Text Input

  * Abstract

    * This paper presents TypingRing, a wearable ring platform that enables text input into computers of different forms, such as PCs, smartphones, tablets, or even wearables with tiny screens. The basic idea of TypingRing is to have a user wear a ring on his middle finger and let him type on a surface – such as a table, a wall, or his lap. The user types as if a standard QWERTY keyboard is lying underneath his hand but is invisible to him. By using the embedded sensors TypingRing determines what key is pressed by the user. Further, the platform provides visual feedback to the user and communicates with the computing device wirelessly. This paper describes the hardware and software prototype of TypingRing and provides an in-depth evaluation of the platform. Our evaluation shows that TypingRing is capable of detecting and sending key events in real-time with an average accuracy of 98.67%. In a ﬁeld study, we let seven users type a paragraph with the ring, and we ﬁnd that TypingRing yields a reasonable typing speed (e.g., 33 - 50 keys per minute) and their typing speed improves over time.
  * Introduction

    * As computing systems evolve, so do their input methods. With advancements in computing technology, different forms of text input methods have been proposed and used in practice. These forms include the standard QWERTY keyboards for PCs, alphanumeric keypads and small keypads in earlier mobile phones, and on-screen soft keyboards in modern smartphones and tablets. Each of these input methodologies for text have been invented out of the need for a change, as the form factor and the mobility requirements of these devices have also changed. We are now at the forefront of technology where wearable computers, such as smart watches and smart bands, have entered the consumer market. These devices have even smaller screen sizes and none of the existing typing methods are viable for these devices. A quick fix to this problem has so far been in the form of speech-to-text or shared keypads. However, the core problem has still remained unsolved, i.e. there is no typing accessory that is portable and usable with computers ofall form factors and mobility requirements.
    * To meet this need we have created TypingRing, which is a wearable keyboard in the form factor of a ring. A user wears the ring on his middle finger and types in text with three ﬁngers (the index, middle and traditional ring ﬁnger) on a surface, such as – a table, his lap, or a wall. The user types and moves his hand as if there is an invisible standard keyboard underneath his hand. By moving the hand horizontally and vertically, TypingRing distinguishes one region from another on the imaginary keyboard. Further, by pressing one of his three ﬁngers, the user types in the key. By using the embedded sensors surrounding the ring, TypingRing determines what key is pressed by the user completely inside the ring and then sends the key event to a remote computer over a Bluetooth Low Energy (BLE) communication link. TypingRing implements a standard BLE keyboard protocol so that it can be used with commercially available computing devices, such as – PCs, smartphones, tablets, or even wearables with tiny screens that support an external BLE keyboard. A piece of software running on the computing device intercepts key events and provides visual feedback to the user by highlighting a key or a portion of a custom on-screen keyboard, as the user moves his hand on the surface and types in keys.
    * Several salient features when combined together make TypingRing the first of its kind. First, TypingRing being a wearable device, is mobile and portable. The ring comes in handy in scenarios where a quick and on-the-go text input is needed or scenarios when an alternative input method is not convenient, e.g. devices with tiny screens. Second, TypingRing is fast and highly accurate in detecting keys, and it performs all its computations inside the ring – without requiring any computational support from a more capable device. Third, TypingRing is multi-platform. Because of its adoption of a standard BLE keyboard protocol, TypingRing is usable with any computing device that supports an external BLE keyboard. Fourth, typing with TypingRing is intuitive and it is easy to learn. TypingRing breaks down the task of typing on a standard keyboard into two intuitive tasks, i.e. moving a hand on a surface and then pressing a ﬁnger, which require little or no practice to get started with. Fifth, TypingRing is flexible and extensible. It is not tied to the English alphabet or any specific keyboard layout. By changing the mapping between a position and a key, TypingRing is usable with keyboards of different layouts and dimensions.
    * TypingRing brings both engineering and computational challenges in front of us. The hardware architecture of TypingRing is designed to obtain the relative movements of the finger, and horizontal and vertical motions of the hand on a surface, so that thusobtained data can be used to infer the position of the hand and typing gestures from just a single ﬁnger. To realize this, we embed a tiny microcontroller, an accelerometer, multiple line sensors, an optical displacement sensor, and a BLE chip on the perimeter of a circular ring platform. These sensors are read by software running inside the ring, which detects and classiﬁes typing gestures using an offline trained Hidden Markov Model (HMM) classiﬁer. As an additional feature in TypingRing, we have implemented a simple Naï£¡ve Bayesian classiﬁer to infer 3D gestures, such as - pitch, roll, and yaw, and map them to commonly used keys on a keyboard to offer shortcut keys to the user.
    * We have created a prototype of TypingRing using off-the-shelf sensors and an open source miniature hardware platform called TinyDuino [11]. In order to tune various parameters of the system and train the typing and gesture classifiers, we perform an empirical study involving 18 users who use the ring to type in letters, words, lines, and gestures while we store all the raw sensor readings. Based on this empirical data, we measure execution time, energy consumption, and the accuracy of typing and gesture classiﬁers. Our empirical evaluation shows that TypingRing is capable of detecting and generating key events in real-time with an average accuracy of 98.67%. Finally, we perform a ﬁeld study, in which, we let seven users type a paragraph with TypingRing and we ﬁnd that TypingRing yields a typing speed of 0.55-0.81 keys per second, and their typing speed improves over time.
    * The contributions of this paper are the following

      * We introduce TypingRing, which is a wearable, portable accessory device that allows a user to input text into mobile and non-mobile computers of different forms.
      * We describe a Hidden Markov Model (HMM) based typing gesture recognizer that uses acceleration, optical displacement and proximity sensors to infer the typing finger in real-time and with an average accuracy of 98.67%.
      * We perform a field study where seven users type a paragraph with TypingRing and we ﬁnd that TypingRing yields a typing speed of 33 - 50 keys per minute, and their typing speed improves over time.
* FingerSound: Recognizing unistroke thumb gestures using a ring

  * Abstract

    * We introduce FingerSound, an input technology to recognize unistroke thumb gestures, which are easy to learn and can be performed through eyes-free interaction. The gestures are performed using a thumb-mounted ring comprising a contact microphone and a gyroscope sensor. A K-Nearest-Neighbor(KNN) model with a distance function ofDynamic Time Warping (DTW) is built to recognize up to 42 common unistroke gestures. A user study, where the real-time classification results were given, shows an accuracy of 92%-98% by a machine learning model built with only 3 training samples per gesture. Based on the user study results, we further discuss the opportunities, challenges and practical limitations of FingerSound when deploying it to real-world applications in the future.
  * Introduction

    * Wearable computing has developed from a niche to a sizable consumer market that has seen substantial uptake in most recent years. Wearable devices – most prominently smart watches and fitness bands, but also mobile virtual reality devices such as the Oculus Rift – can now be considered commodity hardware and large proportions of the population are using them in their everyday lives. With such popularity comes the desire and opportunity for streamlining input to wearable and mobile computing devices. The reason for the former is that traditional means of interaction, such as mouse and keyboard, are typically not very well suited for miniaturized mobile and wearable devices. On the other hand, progress in miniaturization and substantially increased sensing and computing capabilities enables innovative and potentially more convenient means of interaction. Furthermore, with the emergence of entirely new device categories and / or application domains effective input means need to be developed that are both convenient for the user and reliable to process automatically.
    * In the example of mobile virtual reality (VR), users are immersed in a synthetic world where traditional computer interfaces such as the keyboard and mouse may disappear. As such, the demand for novel, effective input modalities is striking. Arguably, text input for short messages may still be necessary in such scenarios for responding to notifications from others, labeling files or objects, or controlling the operating system. The need to input short messages input has led to VR systems that render virtual keyboards and controls over the virtual world where the user can select each letter with head or hand movement. Another option is to render a representation of a physical keyboard in the virtual world so that the user can find it. Both of these options require significant visual and manual attention and can break the sense of immersion in the virtual world. In addition, physical keyboards (and virtual keyboards rendered in a specific location) require the user to move to the interface, which may be awkward or distracting in a virtual world.
    * In this paper we present FingerSound, a system for unistroke thumb gesture recognition that enables characterbased input for wearable computing devices. FingerSound uses a ring with an gyroscope and a contact microphone on the thumb to detect unistroke gestures made against the hand. A user can perform gestures by rubbing/scraping the thumb across fingers. Input can be started virtually at any time and in any position without requiring the visual attention of the user to select each letter. Similarly, command gestures can be made without requiring the visual attention of the user or causing the user to feel around the physical environment blindly searching for an interface device.
    * FingerSound may also be useful in certain contexts for wearable computing. Imagine being in a team meeting with a head worn display integrated into the lens of a pair of eyeglasses [18]. Unlike the use of a mobile phone in a meeting, the head-worn display is designed to be subtle and maintain the rapport of the conversation. However, as soon as the user touches the eyeglasses to control them, it draws attention to the wearer and to the use of the system. With FingerSound, the user can place their hand under the table and give commands to the head worn display. Suppose the head worn display shows an incoming phone call. Drawing an X with the thumb against the palm sends the call to voice-mail. Similarly, an incoming text message might show options for quick responses that the user selects by drawing a number or letter against his palm. For example, “meet you for dinner¿‘ might show “K: OK X: Can’t make it“ and the user selects ‘OK‘ by drawing a K against their palm. For situations where a custom and short message needs to be constructed, FingerSound allows all 36 letters and digits to be written by scraping the thumb across the fingers. While FingerSound is limited to writing words character by character, today’s auto-completion systems can help speed text input and correct recognition and spelling errors in the future.
    * In this paper, we demonstrate different sets of easily learned unistroke gestures (e.g., directional controls, the digits 0−9, and Graffiti characters) as Figure 1 shows, that can be performed by the wearer subtly and without the need to look at the device. The arrow shows the movement of the thumb. Moreover, it does not cause any social awkwardness that may result from using other wearable devices, like Google Glass. In summary, FingerSound presents the following contributions:

      * The design of a ring with a built-in contact microphone and a gyroscope sensor, that captures the sound and movement of a thumb drawing gestures along the fingers.
      * The demonstration of three sets of unistroke-based thumb gestures.
      * A data processing pipeline that uses K-Nearest-Neighbors for classification and Dynamic Time Warping as a temporal distance metric for gesture classification.
      * A user study to validate the effectiveness of FingerSound in recognizing three sets of unistroke gestures using only three training samples per gesture.
      * A discussion of the opportunities and challenges for real-world deployment.
* BiTipText: Bimanual Eyes-Free Text Entry on a Fingertip Keyboard

  * Abstract

    * We present a bimanual text input method on a miniature fingertip keyboard, that invisibly resides on the first segment of a user’s index finger on both hands. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The design of our keyboard layout followed an iterative process, where we first conducted a study to understand the natural expectation of the handedness of the keys in a QWERTY layout for users. Among a choice of 67,108,864 design variations, we identified 1295 candidates offering a good satisfaction for user expectations. Based on these results, we computed an optimized bimanual keyboard layout, while considering the joint optimization problems of word ambiguity and movement time. Our user evaluation revealed that participants achieved an average text entry speed of 23.4 WPM.
  * Introduction

    * As computing becomes ubiquitous, the need to provide users with a fast, subtle, and always-available mechanism for text entry has grown significantly. Micro thumb-tip gestures can deliver on this promise, by allowing a user to type by tapping on a miniature keyboard residing invisibly on the first segment of an index finger, using the thumb (TipText [38]). Thus, the text input can be carried out unobtrusively and even without the user looking at the keyboard (referred to as “eyes-free” in this paper). This can lead to better performance when compared with eyes-on input [44] and can also save screen real estate for devices with limited screen space. However, the existing technique is exclusively unimanual [38], despite typing often being a two-handed activity.
    * In this paper, we propose a keyboard design for bimanual thumb-tip text input. With two index fingers, the size of the input space doubles, thus the keys are larger and less crowded, which is helpful for reducing tapping errors. Additionally, keys residing on different index fingers use two separate input spaces, thus they can no longer be confused with each other by the system. This largely mitigates the ambiguity issue that is inevitable on a miniature fingertip keyboard. Further, the handedness of the keys on a layout (i.e. which index finger a certain key resides on) determines how typing alternates between the two hands (e.g., left -> left -> left -> right -> left). For words with a unique order of handedness, they cannot be typed incorrectly as long as the user types the words in the correct order of handedness, even if the keys are not tapped precisely. All of these benefits can make typing more accurate and faster.
    * Despite these benefits, identifying an optimized design for a bimanual layout for eyes-free typing is challenging for several reasons. First, it is unclear how to optimize the keyboard layout for an improved typing speed to get the most out of the key handedness unique to the bimanual text input. Second, numerous design options exist for a bimanual thumb-tip keyboard, but it is unlikely to conduct user studies to test every possible layout variation to find an optimized design. Third, the performance of the layouts may vary significantly even with small changes but there is a lack of a mechanism that can effectively measure how well a certain design may work in comparison to millions of other alternatives.
    * To explore the design space of this new text entry technique (which we call BiTipText), we took an iterative design approach, where we first conducted a study to understand a users’ natural expectation of the handedness of the keys in a QWERTY layout. Among the choice of 67,108,864 possibilities, we identified 1295 candidates offering good satisfaction on user expectation. The results were used for our layout optimization, where we performed a stepwise search for optimized layout variations and identified one that improves the movement time (target acquisition speed) and word ambiguity (Figure 1c). Finally, we optimized this design for eyes-free input by utilizing a spatial model reflecting a users’ natural spatial awareness of key locations on the tip of the index fingers. To evaluate our technique, we conducted a study with 10 participants to evaluate the speed and accuracy of BiTipText in a controlled experiment. Our results revealed that participants could achieve an average of 23.4 WPM with 0.03% uncorrected errors.
    * Our contribution is two-fold: an optimized keyboard layout design for bimanual thumb-tip text input and a user study demonstrating the effectiveness of BiTipText.
* ARKB: 3D vision-based Augmented Reality Keyboard

  * Abstract

    * In this paper, we propose a wearable 3D Augmented Reality Keyboard (ARKB) which enables a user to type text or control CG objects without using conventional interfaces, such as keyboard or mouse. The proposed ARKB exploits 3D depth information obtained through a stereo camera attached to an HMD. The ARKB consists of three modules: (i) 3D vision-based tracking, (ii) natural interaction with fingers, and (iii) audiovisual feedback on the 3D video see-through HMD. The proposed ARKB can be applied as an interface for typing in AR environment. The remaining challenges are study on tracking method to improve accuracy and newly designed virtual keyboard which is proper in representing the advantage of the interaction in 3D space.
  * Introduction

    * As computing environment changes, new interfaces have been introduced to provide natural interaction between human and computers [1]. A portable keyboard is the first system to improve the disadvantage of general keyboard that is cumbersome to carry [2]. However, it is hard to use these devices with wearable computers. To resolve this problem, the concept of virtual keyboard is introduced. Virtual keyboard is defined as a touch-typing device which does not have physical state of the sensing area [3]. That is, sensing area is not real. So, a virtual button works as a button. Thus, the sensing areas are recognized by high efficiency finger tracking methods such as photoelectric sensor, high efficiency finger tracking method, or touch pads, etc.
    * Figure 1(a) shows SCURRY [4]. It detects movements of fingers and wrist by employing gyro sensors attached on user’s fingers. It recognizes key inputs by detecting and analyzing the movements. Figure 1(b) shows VKB (Virtual Keyboard) [5]. It projects a virtual keyboard on any flat surface by using a laser diode. Then, it recognizes the interaction between user’s finger and projected images by using an IR camera. Senseboard, shown in Figure1(c), includes two sensors made of combination of rubber and plastic [6]. It recognizes typing by analyzing the data from the sensors attached to user’s palm. Figure 1(d) shows Key-glove[7]. It recognizes signal of data glove which changes according to the movements of user’s fingers.
    * As explained, the recently introduced wearable virtual keyboards needs to have some kinds of special sensors to detect movements of users’ hand [4][5][6]. This limits user’s free movement. In addition, sensors attached on the body may distract users from concentrating their original tasks [7]. Furthermore, these are not suitable for a wearable computer with a video-see through HMD. An input device which works similar to the virtual keyboards can be constructed by using a camera, which attached to the video-see through HMD.
    * In this paper, we propose Augmented Reality Keyboard (ARKB), a novel and convenient wearable keyboard, for the next generation wearable computers with a video see-through HMD. It provides a natural interface. The proposed ARKB recognizes fiducial markers captured using a stereo camera on a video see-through HMD and calculates 3D position and orientation of the markers by exploiting ARToolKit. Then, the system augments and tracks a virtual keyboard on 3D space in front of a user. At the same time, ARKB also detects and tracks the user’s fingertips by using the color markers attached on each fingertip. We assume that a collision occurs when a number of cloud points are contained in a volume space of virtual key. As a result, ARKB tosses corresponding character as an input of audiovisual feedback module. Through audiovisual feedback, ARKB can provide more realistic experience to a user.
    * The proposed system monitors the collision between fingers and augmented keyboard without using any physical sensors. A virtual keyboard is displayed regardless of table color and does not need additional space because it is displayed on HMD worn by the user. ARKB provides natural interaction by exploiting 3D information of a fiducial marker and user’s hand. The proposed system provides natural feeling by feeding back through virtual monitor and speakers.
    * This paper is organized as follows: In Section 2, we describe the proposed ARK in more detail. Some preliminary experimental results and discussions are followed in Section 3 and 4, respectively.
* DigiTouch: Reconfigurable Thumb-to-Finger Input and Text Entry on Head-mounted Displays

  * Abstract

    * Input is a significant problem for wearable systems, particularly for head mounted virtual and augmented reality displays. Existing input techniques either lack expressive power or may not be socially acceptable. As an alternative, thumb-to-finger touches present a promising input mechanism that is subtle yet capable of complex interactions. We present DigiTouch, a reconfigurable glove-based input device that enables thumb-to-finger touch interaction by sensing continuous touch position and pressure. Our novel sensing technique improves the reliability of continuous touch tracking and estimating pressure on resistive fabric interfaces. We demonstrate DigiTouch’s utility by enabling a set of easily reachable and reconfigurable widgets such as buttons and sliders. Since DigiTouch senses continuous touch position, widget layouts can be customized according to user preferences and application needs. As an example of a real-world application of this reconfigurable input device, we examine a split-QWERTY keyboard layout mapped to the user’s fingers. We evaluate DigiTouch for text entry using a multi-session study. With our continuous sensing method, users reliably learned to type and achieved a mean typing speed of 16.0 words per minute at the end of ten 20-minute sessions, an improvement over similar wearable touch systems.
  * Introduction

    * Head-mounted displays (HMDs) for wearable virtual reality (VR) and augmented reality (AR) systems have seen a recent resurgence in interest and popularity. Fueled by advances in display and embedded technologies, head-mounted displays are poised to impact the way we work, play, and communicate. Currently, consumer applications of these technologies are focused on gaming and entertainment in stationary environments. Existing input techniques require socially awkward interactions or instrumented environments, limiting the broader usage of these devices in mobile settings. For example, head-mounted touch interfaces (Google Glass, Samsung Gear VR) or in-air gesture interfaces (Microsoft HoloLens) require raising a hand to eye level, which can be tiring and draw unwanted attention to the user. Speech input (Google Glass, Microsoft HoloLens) is useful for dictation and simple navigation, but may be disturbing to others and is not always socially acceptable [14, 33]. High-end VR devices (HTC Vive, Oculus Rift) use handheld positionally-tracked controllers for input. While these controllers offer immersive gaming experiences, they are not always appropriate for mobile input, as they require the user to hold an extra device. Therefore, to enable broader use of head-mounted computing devices, there is an unmet need for input methods that are expressive, subtle, and portable.
    * Thumb-to-finger interaction is a promising technique that can be performed discreetly, without large hand movements. Placing the input surface on the fingers enables fine-grained control that leverages both tactile and proprioceptive feedback. Furthermore, Huang et al. have shown that thumb-to-finger interactions are both comfortable and highly accurate [11]. Unlike many input methods that demand a particular posture during use, one can subtly swipe along a finger with the thumb with the arms at rest.
    * Though traditional optical hand trackers excel at hand pose detection and offer augmentation-free tracking, they do not provide enough granularity to precisely detect finger touch events and positions. Gloves, however, offer a number of advantages: freedom from occlusion and lighting problems may allow more subtle use and gloves may have fewer errors in recognizing input events. Gloves are particularly well-suited for input outside of traditional desktop computing environments (e.g. on the bus) or in situations where gloves are already commonly used (e.g. outdoors in cold weather). Because of the difficulty in creating non-contact haptics, people may also wear gloves when haptic feedback, such as force feedback or vibrotactile feedback, is desired. Though gloves might not be socially-acceptable in some situations, there is no one-size-fits-all input solution for head-mounted displays, and the ability to choose from a range of input devices for the situation will help make HMDs more ubiquitous.
    * In this paper, we present DigiTouch, a touch-sensitive glove that enables thumb-to-finger interaction for eyes-free input on wearable systems. DigiTouch uses thin, partially conductive fabric strips along the fingers and a conductive patch on the thumb pad (Figure 1). Each strip can sense the continuous touch position and pressure of the thumb as it touches the finger. This enables precise, yet subtle input through tapping, sliding, force-pressing, and two-handed chording gestures (Figure 2).
    * Unlike other data gloves [15, 16, 27, 32], which use only discrete touch regions, DigiTouch senses the continuous touch position of the thumb. This capability makes it reconfigurable; allowing it to be used for various tasks like target selection, slider control, and text entry. Depending on application requirements, different widgets of varying size can be mapped to different regions of a particular finger. Though others have demonstrated fabric-based touch interfaces that sense continuous input, such systems either require multiple layers of fabric [8] that hinder tactile feedback, or use sensor arrays [29] making them bulky and complex. It is also unclear how well these systems operate when bent or stretched, as doing so can change the electrical properties of fabrics. To overcome these challenges, we present a new technique for continuous sensing on fabric that uses only a single layer of fabric and a two-wire interface on each finger. DigiTouch accounts for the variable resistance as the fingers bend using current monitoring and time-multiplexed sensing.
    * DigiTouch is a general-purpose input device for AR/VR systems and can be used for different applications (Figure 2). For example, a user can dial a number using a ten-digit numeric keypad, move a virtual object by sliding along a finger, or control an application using any combination of buttons and sliders along the fingers. However, in evaluating DigiTouch, we decided to place special emphasis on text input using a split-QWERTY keyboard. The reason is three-fold: (1) Text entry is a challenging task in today’s AR/VR systems and is a barrier to enabling more productive use-cases. (2) There is limited quantitative data on the evaluation of such wearable input systems for AR/VR applications, and text entry provides a well-established set of quantitative measures that helps in formalizing the system’s performance. (3) The high density of keys using a full QWERTY keyboard in a fixed 2-dimensional space makes text entry a challenging task for the user, and a rigorous test for the usability and practicality of DigiTouch. We directly map a split-QWERTY keyboard layout to a user’s fingers, as shown in Figure 2 (right). This closely resembles the two-thumb typing posture on a smartphone. From a longitudinal study with ten participants, we found that the participants quickly learned how to use DigiTouch for entering text. Their mean typing speed increased from 7.0 wpm (words per minute) to 16.0 wpm in 10 twenty-minute sessions. The participants also achieved a mean uncorrected error rate of 0.85% on the last session.
    * The main contributions of our work are:

      * A reconfigurable touch-sensitive glove that senses continuous touch position and pressure, enabling thumb-to-finger interactions for wearable computing.
      * A text entry system using thumb-to-finger interactions based on a split-QWERTY keyboard.
      * A quantitative evaluation of the text entry capabilities of DigiTouch using a ten-session study with 10 participants.
* TipText: Eyes-Free Text Entry on a Fingertip Keyboard

  * Abstract

    * In this paper, we propose and investigate a new text entry technique using micro thumb-tip gestures. Our technique features a miniature QWERTY keyboard residing invisibly on the first segment of the user’s index finger. Text entry can be carried out using the thumb-tip to tap the tip of the index finger. The keyboard layout was optimized for eyesfree input by utilizing a spatial model reflecting the users’ natural spatial awareness of key locations on the index finger. We present our approach of designing and optimizing the keyboard layout through a series of user studies and computer simulated text entry tests over 1,146,484 possibilities in the design space. The outcome is a 2×3 grid with the letters highly confining to the alphabetic and spatial arrangement of QWERTY. Our user evaluation showed that participants achieved an average text entry speed of 11.9 WPM and were able to type as fast as 13.3 WPM towards the end of the experiment.
  * Introduction

    * As computing devices are being tightly integrated into our daily living and working environments, users often require easy-to-carry and always-available input devices to interact with them in subtle manners. One-handed micro thumb-tip gestures offer new opportunities for such fast, subtle, and always-available interactions especially on devices with limited input space (e.g., wearables) [3]. Very much like gesturing on a trackpad, using the thumb-tip to interact with the virtual world through the index finger is a natural method to perform input. This has become increasingly practical with the rapid advances in sensing technologies, especially in epidermal devices and interactive skin technologies [71, 72]. While many mobile information tasks (e.g., dialing numbers) can be handled using micro thumb-tip gestures [32], text entry is overlooked, despite that text entry comprises of approximately 40% of mobile activity [10].
    * Using the thumb-tip for text entry on the index finger has several unique benefits. First, text input can be carried out using one hand, which is important in mobile scenarios, as the other hand can be occupied by a primary task. Second, text input can be carried out unobtrusively, which can be useful in social scenarios, such as in a meeting, where alternative solutions, like texting on a device (e.g., smartphone or watch) or using speech may be socially inappropriate or prone to exposing the users’ privacy. Third, text input can be carried out without looking at the keyboard (referred to as “eyes-free” in this paper). This can lead to better performance than eyes-on input [82] and save screen real estate for devices with a limited screen space.
    * Despite all these benefits, eyes-free text entry using the thumb-tip and the index finger is challenging because of the lack of input space, missing proper haptic feedback, and lack of a flat and rigid surface on the index finger. A QWERTY keyboard can barely be laid out on the index finger and the keys can be too small to type. Unlike a physical keyboard, typing on the index finger offers little useful haptic feedback to inform the user about which key was selected, making it more difficult for eyes-free typing. Finally, the tip of the index finger is curved and soft, which may impact tapping accuracy on those already small keys.
    * In this paper, we present TipText, a one-handed text entry technique designed for enabling thumb-tip tapping on a miniature fingertip keyboard on the index finger. TipText features a QWERTY keyboard, familiar to most of today’s computer users, in a 2×3 grid layout residing invisibly on the first segment of the index finger (Figure 1c). The design of the grid layout was optimized for eyes-free input by utilizing a spatial model reflecting the users’ natural spatial awareness of key locations on the index finger. The efforts of learning to type with eyes-free is largely minimized.
    * We explored the design space of this new text entry technique in a wide spectrum of design options, ranging from the default QWERTY layout with 26 keys to layouts with a lower number of keys that are larger keys to facilitate tapping. Among the choices of 1,146,484 possibilities, we struck a balance between layout learnability, key size, and word disambiguation introduced by associating the keys with more than one letter. Through a number of user studies and computer simulated typing tests, we compared the performance of various design options and identified an optimized design for TipText. Lastly, we conducted a controlled experiment to evaluate the speed and accuracy of TipText using a proof-of-concept interactive skin overlay placed on the tip of participants’ index fingers. Our results revealed that participants could achieve an average of 11.9 (s.e. = 0.5) WPM with 0.30% uncorrected errors.
    * In summary, our contributions are: (1) a spatial model workable with thumb-tip tapping on fingertip surface (e.g. interactive skin); (2) an optimized keyboard layout design for TipText; and (3) a user study demonstrating the effectiveness of TipText.
* QwertyRing: Text Entry on Physical Surfaces Using a Ring

  * Abstract

    * The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea ofQwertyRing is to detect touch events and predict users’ desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.
  * Introduction

    * Text entry for head-mounted displays (HMDs), such as AR and VR headsets requires improvement. Existing approaches for text entry leverage head rotation [73], mid-air gesture [24], or swiping on the headset [20, 74] for inputting text. However, text entry techniques based on head rotation and mid-air gesture create fatigue, while swiping on the headset to input text is not efficient enough (<10 WPM). These solutions are not user friendly and efficient compared to the widely used software keyboard on smartphones, which is based on "touch" - the most frequently used input method. In our research, we attempted to support touch-based text entry on any physical surface using a single wearable device.
    * We present QwertyRing, a text entry technique using a finger-worn 6-axis IMU. QwertyRing enables text entry on any desk-like surface, which is rigid, flat and spacious enough to accommodate a keyboard. Users enter characters and select the desired words from word suggestions. To use QwertyRing, users wear the ring on the middle phalanx of the index finger and receive visual feedback from an external display, e.g., an AR/VR headset or a smart TV. Users rest the wrist on the interaction surface and imagine that there is a QWERTY keyboard in the reachable area of the index finger, and then type. The system detects touch events on the surface and predicts users’ desired words according to the orientation of the IMU at touch moments.
    * QwertyRing has multiple inherent advantages. First, QwertyRing gathers data exclusively from an IMU sensor, which is affordable to produce and low power consumption. Second, QwertyRing is convenient to use. The device is wearable and can be used on any desk-like surface. Third, typing with QwertyRing requires no visual attention to the hand. Users can focus on the display, which is important in VR and smart TV scenarios [35]. Finally, QwertyRing is easy to learn: it enables direct touch input on the QWERTY layout, which retains users’ muscle memory based on daily use of smartphone typing.
    * In literature, the feasibility of typing on physical surfaces using a ring is unexplored. There are some difficulties. First, 2D finger tracking using only a ring is often not accurate (error > 1 cm), even with personalized calibration [29]. Second, while accurate detection of touch down on physical surfaces using a ring was proposed recently [21], the detection of more touch events including touch up, long press and swiping were unexplored. To address these gaps and build QwertyRing, we conducted three user studies, each contributing to answer one of the three research questions as follows:

      * RQ1): how to detect touch events on physical surfaces using a ring? We conducted the first study to collect users’ data of touch interactions including tapping, long presses and swiping. Using this data, we followed prior work [21] to detect touch down on physical surfaces and proposed our method to detect touch up event. The accuracies (F1 score) were 99.30% and 99.50% respectively. Based on the sensing of touch down and touch up, the system can accurately recognize other touch events, including long press, swiping left and swiping right by threshold methods.
      * RQ2): what is an optimal design for the text entry decoder? We conducted the second study to collect users’ typing data. Based on the data, we analyzed the user behavior of typing and designed the decoder of QwertyRing. Combined with a trigram [28] language model, the decoder reached a top-1 accuracy of 79.0% and a top-5 accuracy of 94.6% in simulation. The result shows that the decoder could support text entry with word suggestions. Besides, there was a large difference in user behavior, which inspired us to explore the personalization.
      * RQ3): how fast can a user type with QwertyRing? We conducted the third study to evaluate the performance ofQwertyRing with the general model and the personal model through a five-day long, between-subject study. Both the two models (decoders), were fitted by all of the data in the second study. The personal model updated every day using existing personal data in the third study, while the general model did not change over days. The result shows that both of the two models performed well. In average, participants typed at a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM with a five-day training. There was a trend that the personal model decoded better than the general model from day three on (p = 0.09).
    * The contributions of this work are three-fold:

      * We support touch-based and QWERTY-based text entry on any physical surface using a single wearable device.
      * We follow prior work [21] to sense touch down on physical surfaces using an IMU ring and supplement it with the touch up sensing. The accuracies are higher than 99%. We further detect touch events including long press, swiping left and swiping right.
      * We empirically demonstrate that QwertyRing is efficient, accurate and easy to use.
* ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data

  * Abstract

    * Ten-finger freehand mid-air typing is a potential solution for post-desktop interaction. However, the absence of tactile feedback as well as the inability to accurately distinguish tapping ﬁnger or target keys exists as the major challenge for mid-air typing. In this paper, we present ATK, a novel interaction technique that enables freehand ten-ﬁnger typing in the air based on 3D hand tracking data. Our hypothesis is that expert typists are able to transfer their typing ability from physical keyboards to mid-air typing. We followed an iterative approach in designing ATK. We ﬁrst empirically investigated users’ mid-air typing behavior, and examined ﬁngertip kinematics during tapping, correlated movement among ﬁngers and 3D distribution of tapping endpoints. Based on the ﬁndings, we proposed a probabilistic tap detection algorithm, and augmented Goodman’s input correction model to account for the ambiguity in distinguishing tapping ﬁnger. We ﬁnally evaluated the performance of ATK with a 4-block study. Participants typed 23.0 WPM with an uncorrected word-level error rate of 0.3% in the ﬁrst block, and later achieved 29.2 WPM in the last block without sacriﬁcing accuracy.
  * Introduction

    * Mid-air text entry is a potential and promising solution for post-desktop interactions such as virtual reality [13], large displays [19], and even mobile phones [24]. To achieve this, a number of techniques have been proposed, including those using data gloves to capture hand/finger movement [5, 13], and those based on target selection [4, 20] and gestures [23] using only one or two ﬁngers. However, no one has researched ten-ﬁnger freehand mid-air typing. We deem it important because ten-ﬁnger typing is the most effective means that human enters text in daily lives; meanwhile, with recent development of hand tracking techniques (e.g. LeapMotion and Kinect), such vision should come true.
    * However, issues arise when users perform ten-finger typing in mid-air. There is no tactile feedback to facilitate users’ tapping action or hand/ﬁnger navigation on the virtual keyboard, nor is there a mechanism to explicitly detect human’s tapping action on individual keys as that on a physical keyboard. Therefore, the algorithm has to infer users’ input intention from the continuous 3D hand/ﬁnger movement data.
    * In this paper, we describe our iterative approach in designing ATK (Air Typing Keyboard, pronounced “attack”), a novel technique that enables freehand ten-finger mid-air typing. We ﬁrst empirically investigated users’ ten-ﬁnger mid-air typing pattern, including ﬁngertip kinematics during tapping, correlated movement among ﬁngers and 3D distribution of tapping endpoints. Based on the results, we proposed a probabilistic tap detection algorithm, and augmented Goodman’s input correction model. Finally, we evaluated our prototype with 8 participants in 4 blocks. Results showed that with ATK, participants could achieve 23.0 WPM with an uncorrected word-level error rate of 0.3% at ﬁrst, and later achieved 29.2 WPM after some practice without sacriﬁcing accuracy; ATK surpassed existing mid-air text entry techniques in terms of both speed and accuracy.
* FingerT9: Leveraging Thumb-to-finger Interaction for Same-side-hand Text Entry on Smartwatches

  * Abstract

    * We introduce FingerT9, leveraging the action of thumb-tofinger touching on the finger segments, to support same-sidehand (SSH) text entry on smartwatches. This is achieved by mapping a T9 keyboard layout to the finger segments. Our solution avoids the problems of fat finger and screen occlusion, and enables text entry using the same-side hand which wears the watch. In the pilot study, we determined the layout mapping preferred by the users. We conducted an experiment to compare the text-entry performances of FingerT9, the tilt-based SSH input, and the direct-touch nonSSH input. The results showed that the participants performed significantly faster and more accurately with FingerT9 than the tilt-based method. There was no significant difference between FingerT9 and direct-touch methods in terms of efficiency and error rate. We then conducted the second experiment to study the learning curve on SSH text entry methods: FingerT9 and the tilt-based input. FingerT9 gave significantly better long-term improvement. In addition, eyes-free text entry (i.e., looking at the screen output but not the keyboard layout mapped on the finger segments) was made possible once the participants were familiar with the keyboard layout.
  * Introduction

    * The smartwatch is emerging as a major category of personal computing devices after the desktop PCs, laptops, smartphones, and tablets. There are various smartwatch applications, such as checking emails, calling, messaging, and social networking. Among these applications, typing/text entry is essential [36]. Traditionally, text entry techniques for small displays employ QWERTY-like soft keyboards [20]. Several novel text-entry methods, such as multiple tap selection [23] and memorization of individual gestures [9], have been proposed to facilitate touch-based smartwatch text entry. However, touching on smartwatch usually requires the input from the non-wearing hand, and this may not be feasible when the non-wearing hand is occupied by other tasks. Voice input is an alternative input method. However, it may become awkward in certain situations, e.g., due to privacy or noisy environment. On the other hand, users often adopt one-handed strategies with their thumbs to interact with mobile devices when his/her other hand is occupied. There is still lack of efficient one-handed (same-sided hand) technique that particularly aims to address the problem of text entry on smartwatches.
    * Research showed that people can accurately touch their finger segments with their thumbs and thumb-to-finger interfaces support effective eyes-free interaction [16]. In this paper, we introduce FingerT9, leveraging the action of thumb-to-finger touching on the finger segments, to support same-sided-hand (SSH) text entry on smartwatches (Figure 1a). FingerT9 contributes the first design and empirical investigation on the same-sided hand (SSH) smartwatch text entry, an important but underexplored problem. SSH interaction can offer benefits by freeing the other hand for tasks like carrying a bag, and allow users to operate mobile devices in a distracted, multitasking scenario. While the existing smartwatch text input methods are fast, they require the other hand for input and thus cannot be used in these scenarios. In FingerT9, a T9 phone keyboard [12] is mapped onto the finger segments (Figure 1b). The T9 layout was chosen due to its common usage especially among feature phone users and the intuitive mapping between the T9 keyboard and the finger segments. We developed an experimental prototype of FingerT9 by attaching thin capacitive touch sensors to the finger segments (Figure 2) and algorithmically predicting the user’s intention based on a series of thumb-to-finger taps.
    * We conducted a controlled experiment to compare FingerT9, a tilt-based SSH interaction method for text entry, and the direct-touch text entry which uses the T9 keyboard layout but requires the input from the non-wearing hand. The results showed that the participants performed significantly faster and more accurately with FingerT9 (WPM: 3.43, error rate: 11.14%) than the tilt-based text-entry (WPM: 2.45, error rate: 20.73%). While the participants typed significantly faster with the direct-touch input method (WPM: 6.50) than FingerT9, there was no significant difference between these two methods in terms of efficiency and error rate, and FingerT9 could outstand in the one-hand situation. A 5-day user study further revealed that FingerT9 yielded significantly better long-term improvement than the tilt-based method. With FingerT9, the users achieved 5.42 WPM with an error rate of 4.68% after a 5-day training.
    * Our contributions are two-fold:

      * The integration of thumb-to-finger interaction with text entry on smartwatches;
      * The evaluation that showed the advantages of FingerT9 over a tilt-based SSH text-entry method.
* Huffman Base-4 Text Entry Glove (H4-TEG)

  * Abstract

    * We designed and evaluated a Huffman base-4 Text Entry Glove (H4-TEG). H4-TEG uses pinches between the thumb and fingers on the user’s right hand. Characters and commands use base-4 Huffman codes for efficient input. In a longitudinal study, participants reached 14.0 wpm with error rates <1%. In an added session without visual feedback, entry speed dropped only by 0.4 wpm. Yet another session was added that required entry of text with punctuation and other symbols. Entry speed dropped by 1.5 wpm.
  * Introduction

    * The human body is able to perform numerous discrete actions. Many actions do not require visual attention and can be performed with ease [18]. A skilled guitarist, for example, plays musical notes effortlessly due to honed muscle memory combined with tactile feedback from the guitar’s neck, strings, and frets. There are many ways to leverage the human ability to learn precise body movements and incorporate them into wearable computing.
    * Recent mobile trends show that users are as inclined to text each other as to call and talk [8]. As the mobile technology moves closer to wearable computing, texting techniques will have to migrate as well.
    * In this paper, we describe a one-handed text input technique that makes use of a pinch activated glove called Huffman base-4 Text Entry Glove (H4-TEG). Our goal was to reduce or eliminate the need for visual attention and to retain high efficiency. After reviewing related work, we present H4-TEG, which incorporates the H4-Writer algorithm [13]. Results of a longitudinal evaluation are then presented.
* RotoSwype: Word-Gesture Typing using a RingD

  * Abstract

    * We propose RotoSwype, a technique for word-gesture typing using the orientation ofa ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1%, outperforming previous comparable techniques.
  * Introduction

    * Text-entry for head-mounted devices (HMDs) for augmented reality (AR), virtual reality (VR) remains an open problem. Existing approaches introduce solutions that use hand-held controllers [22, 63] or swiping on smartglasses [1, 25, 82]. However, hand-held controllers are not feasible for AR, especially in mobile scenarios. Swiping on smartglasses solves that problem, but current techniques report up to 10WPM for expert use, which requires improvement. A second problem with these approaches is that the techniques are optimized for a dedicated device and do not allow for cross-device textinput. Controllers work best with VR, swiping on the edge of smartglasses is specific to touch-sensitive AR HMDs, and none of these techniques can work with other smart devices such as a distant screen or a smart-watch or an IOT (Internetof-Things) device. An ideal technique would be one which can be useful across these device scenarios. While speech is an obvious solution, it can be error-prone for certain users and situations, and can be socially awkward.
    * We propose RotoSwype, a technique that inputs text using a ring in mid-air. RotoSwype uses ring-motion, and consequently wrist-motion, to draw a motion trace over the keyboard, which types a word. Shape-writing or wordgesture typing (WGT) was introduced by Kristensson and Zhai [40, 41, 83] wherein users write a word by tracing a path traversing the letters on the keyboard using a stylus or a finger. Instead of a finger, RotoSwype uses the ring’s tiltmotion to trace a path over the keyboard. RotoSwype is the first ring-based technique that enables word-gesture typing. RotoSwype has the following characteristics: a) Unencumbered: RotoSwype does not require a hand-held device, but a finger-worn ring with a simple motion sensor that allows a miniature design. However, it is not completely freehand [7], given the ring, and therefore we term its use as unencumbered. b) One-handed: While typing, the input is completely one-handed, thus allowing greater flexibility in on-the-go scenarios or when the second hand is encumbered. c) Selfcontained: RotoSwype is self-contained and does not need a surface to provide input, enabling the user to type in mid-air, in any posture, sitting or standing. d) Eyes-away: RotoSwype does not need the user to pay attention to the text-input device, freeing them to look at the keyboard, wherever it may be. e) Any-qwerty input: Using word-gesture typing along with a qwerty keyboard means that RotoSwype can work with any existing qwerty keyboard on any device including HMDs for AR/VR. Given the lock-in effect ofqwerty today [8], it is essential for a text-entry technique to support qwerty-based typing.
    * In this work, we design and implement RotoSwype to work with a standard Android keyboard. We support two hand postures: when the hand is above the waist; and when it is below the waist, relaxed. Since ring orientation changes completely for these two postures, we design two slightly different styles of text-entry for the two postures, which we refer to as - Hand-Up and Hand-Down. For Hand-Up, we further compare for performance between two different mappings of ring orientation to the keyboard plane. Finally, we conduct a five-day study for both Hand-Up and HandDown postures with a HMD. The results show that with <60 mins of typing, both postures result in a speed of >14 WPM, with a near 1% uncorrected error rate, outperforming current techniques that do not use a hand-held controller. The results further showed no signs of plateauing indicating the scope for further improvement.
    * We make the following contributions: 1) Propose and implement a ring-tilt based word-gesture typing technique that enables unencumbered, one-handed qwerty typing for HMDs (and other devices). 2) Demonstrate using a 5-day study that RotoSwype is fast, accurate, and easy to learn.
* ThumbText: Text Entry for Wearable Devices Using a Miniature Ring

  * Abstract

    * Users can benefit from using an auxiliary peripheral that could mitigate many concerns with direct text entry on wearable devices. We introduce ThumbText, a thumb-operated text entry approach for a ring-sized touch surface. Through a multi-part exploration, we first identify a suitable discretization of the miniature touch surface for thumb input. We then design a number of two-step selection techniques for supporting the input of at least 28 characters. On a miniature touch surface, we find that a continuous touch-slide-lift selection technique in a 2×3 grid discretization offers improved performance gains over other selection methods. Finally, we evaluate ThumbText against techniques also designed for wearable devices and find that ThumbText allows for higher text entry rates than SwipeBoard and H4-Writer.
  * Introduction

    * Supporting text entry on wearable devices is still an open challenge [12,13,16,24,32,35,44]. This has led researchers to propose novel approaches to address some of the key text entry challenges on emerging wearables, such as smartwatches [7,14,16,32] and headworn displays (HWDs) [13,38]. However, these approaches are optimized for each device separately and significant design iterations are needed to make them usable across devices [7,13,38]. Ideally one unique text entry mode should be usable across a multitude of wearable devices and thus provide the opportunity to design-and-learn once, and reuse often. Such an approach could mitigate concerns peculiar to any one device, such as finger occlusion [43] and two-handed use on smartwatches, or fatigue [15] and social awkwardness on HWDs [18].
    * One promising solution is the use of wearable text entry peripherals [10,12,20,22,25,31,38]. Such devices allow for indirect input and can be used, if designed appropriately, across more than one wearable display. However, existing peripherals come at a cost. Some require users to hold it, which minimizes the use of an entire hand that can often be indispensable in mobile and wearable contexts [21,39]. In other cases, the peripherals require a steep learning curve, making them unusable for short activity bursts [31,41].
    * For the text entry task on wearable devices, no current peripheral is able to satisfy the following design requirements:

      * miniaturization: the peripheral should ideally be as small as possible to avoid holding it, but yet not too small to detract significantly from its core task;
      * one-handed operation: given the requirements for one-handed operation while on-the-go, users should be able to enter text in one-handed mode;
      * self-contained: for optimum mobility, users should not be required to depend on additional materials or surfaces for text entry;
      * unified input: users should be able to apply the same text entry approach across wearable devices;
    * We propose ThumbText, a peripheral that makes text entry possible via thumb input onto a miniature touchpad affixed to the thumb’s opposing fingers (index or middle finger). As such, ThumbText offers one-handed indirect input, with subtle finger movements [6], and independent of the associated wearable displays. Through a multipart design process, we first assessed the thumb’s dexterity for input on a small touch surface by carefully discretizing the available input space. The ideal discretization patterns were then used to map alphabet characters onto touchpad positions. While operating a smartwatch, we found that ThumbText outperforms SwipeBoard [7] and H4-Writer [27], two very efficient text entry methods applicable to wearable devices.
    * Our contributions include: (1) ThumbText, a peripheral device and its associated text entry method for wearable devices; (b) a multipart design process integrating an understanding of the thumb’s dexterity for text entry with ThumbText; and (c) an evaluation demonstrating ThumbText’s performance in comparison to existing text entry techniques applicable on a miniaturized touch surface.
* FingerPing: Recognizing Fine-grained Hand Poses using Active Acoustic On-body Sensing

  * Abstract

    * FingerPing is a novel sensing technique that can recognize various fine-grained hand poses by analyzing acoustic resonance features. A surface-transducer mounted on a thumb ring injects acoustic chirps (20Hz to 6,000Hz) to the body. Four receivers distributed on the wrist and thumb collect the chirps. Different hand poses of the hand create distinct paths for the acoustic chirps to travel, creating unique frequency responses at the four receivers. We demonstrate how FingerPing can differentiate up to 22 hand poses, including the thumb touching each of the 12 phalanges on the hand as well as 10 American sign language poses. A user study with 16 participants showed that our system can recognize these two sets of poses with an accuracy of 93.77% and 95.64%, respectively. We discuss the opportunities and remaining challenges for the widespread use of this input technique.
  * Introduction

    * Despite years of research and development and substantial progress made, providing appropriate means for input to wearable devices remains a considerable challenge. The size and comfort required for continuous use of a wearable device, as well as the need to operate in mobile contexts with minimal difficulty and attention, make the options of keyboards and touchscreens less desirable. Voice input is one viable alternative, but it is not always the most socially appropriate solution. Another alternative utilizes a user’s phone as input device for their wearable(s). However, such a proxy (or remote control) solution in many cases over-complicates the interaction as it requires the user to first reach for the phone, which is inconvenient in many scenarios such as when the hands are occupied with another task. Furthermore, the user may need to input in a more discreet fashion for privacy and social appropriateness in certain scenarios, such as during a meeting or interacting with smart home devices, such as Google Home or Amazon Alexa. Finally, heads-up displays for augmented and virtual reality present opportunities for non-voice, eyes-free input.
    * Given this motivation for a convenient and socially appropriate wearable input solution, we introduce FingerPing, a novel wrist- and thumb-mounted sensing solution to enable one-handed input. FingerPing relies on detecting various hand configurations (e.g., the thumb touching the tip of a ﬁnger) based on how that conﬁguration impacts the propagation of sound waves injected at the thumb and propagating around the hand. The human body is a good medium for sound propagation [20, 12], and its frequency response varies based on which path a sound wave travels through the body. A change in hand conﬁguration, which results from forming the hand into a variety of different poses or gestures, creates sufﬁciently distinct propagation paths for sound waves. To utilize this phenomenon for recognizing different hand poses, FingerPing injects acoustical chirps (20Hz–6kHz) ten times per second from the base of the thumb. These chirps travel through the hand and are received by microphones present on the thumb and wrist. The received signals are then classiﬁed to match a set of known poses.
    * To demonstrate the capabilities of FingerPing, we designed and evaluated two sets of poses in this paper. The first pose set consists of thumb taps to the 12 phalanges across the four ﬁngers, which can be used for number input and potentially text input with a T9 keyboard1. The other pose set consists of the ten number poses from American Sign Language, further demonstrating the flexibility in reliably distinguishing a large number of simple hand poses. Note that our system effectively detects endpoints of gesture input. However, the actual data analysis –after segmentation– is based on classifying static hand conﬁgurations (poses). Consequently, we denote our approach as pose or hand conﬁguration recognition rather than gesture recognition – even though its purpose is clearly targeting the latter. Our technology may suffer more falsepositive errors caused by the on-body acoustic noise during daily activities. However, false-positives from daily activities can be addressed with a reliable activation pose. The user could perform the activation pose to activate the system which would then enable the full set of poses for recognition. Our technology may also request additional calibration for different users. A user calibration procedure can be adopted to address this issue. More details is provided in the paper.
    * The contributions of this paper are:

      * An active acoustical sensing system that recognizes various hand poses by retrieving and recognizing the acoustical signatures generated on the hand.
      * The design of a one-handed hand poses set, which maps a standard 12-key number pad to the natural structure of the hand and can be used for number input and potentially text input. A second example pose set for input is the set of American Sign Language poses for the ten digits 1–10.
      * An empirical evaluation of the two pose sets, with the results discussed in terms of viability for practical applications.
    * In the remainder of the paper, we will first discuss the previous work and highlight the innovation and contribution of FingerPing. We then present the underlying theory, design and implementation, and empirical evaluation of the system. In the last section, we discuss the challenges and opportunities for using this novel technique in everyday applications.
* A NewWearable Input Device: SCURRY

  * Abstract

    * A new wearable input device named SCURRY, developed by the Samsung Advanced Institute of Technology, is introduced in this paper. Based on inertial sensors, this device allows a human operator to select a specified character, an event, or an operation as the input he/she wants spatially through both hand motion and ﬁnger clicking. It is a glovelike device, which can be worn on the human hand, composed of a base module, including one controller and two angular-velocity sensors (gyroscopes) on the back of the hand, and four ring-type modules (rings), including two-axis acceleration sensors (accelerometers) on four ﬁngers. The base and the ring modules are integrated modules containing sensors, a transceiver or receiver for communication, and a microcontroller, which makes the device compact and light. The two gyroscopes embedded in the base module have a role in detecting the direction (up, down, right, and left) of the handmotion, and the accelerometers have a role in detecting ﬁnger motion generated by ﬁnger clicking. An algorithm for the exact ﬁnger-click recognition composed of three parts (feature extraction, valid-click discrimination, and crosstalk avoidance) is proposed to improve the recognition performance of ﬁnger clicking on SCURRY. The experimental results and discussions are presented. SCURRY can be used as a wearable mouse spatially, by allowing any three ﬁngers to be operated as the left, middle, and right mouse buttons, and in a similar manner, as a wearable keyboard, as it allows a human operator to point and select any character, event, or operation by his hand motion and ﬁnger clicking.
  * Introduction

    * THE RAPID advance of the computing environment increasingly requires a new human–machine symbiosis (human–computing-system symbiosis in this paper). Our primary physical connection to the world is made through our hands.We perform most of our everyday tasks with them.When we work with a computer or the computing system, we are constrained by clumsy intermediary devices such as keyboards, mice, and joysticks. Among these, the keyboard is the most familiar and widely used input device for humans. Although QWERTY-type keyboards are still widely used these days, it is true that they are too bulky and inconvenient for portable computing systems including wearable and mobile computing systems. In addition, a large number of input elements are required to input even a very-small word/phrase vocabulary as these types of keyboards incorporate only one kind of input element—a switch. Since the early 1980s, much attention has been paid to a new portable input device with the physical form of a portable computer [1], [2].
    * In the past, data input requires at least two stages: the initial data processing or collection is carried out via pen and paper in the first stage, and the data are entered into a computer in the second stage. Various wearable input devices allow human operators (users) to remove the ﬁrst conventional stage and reduce the datainput process to a single stage. Some results showed that the introduction of wearable input devices could save task time and reduce error rates by compressing the conventional twostage task into a single stage [3]. Many researches on the evaluation of the ease of learning and using a forearm-mounted keyboard (FK), a virtual keyboard (VK), and a Kordic keypad (KK) [4] in conjunction with a wearable computer followed. The results showed that the FK was the best performer for accurate and efﬁcient text entry, while the others beneﬁted from more work on designing graphic user interface (GUIs) [5]. Other researchers have studied various input devices. The Twiddler input device [6] integrated with keyboard was also introduced. With the device, a human operator can input a text entry as well as the GUI event he wants. Ring and glovelike input devices are introduced, which can input based on the chording combination of the ﬁngers [7], [8].
    * In particular, the development of glovelike input devices has played an important role in human–computing-system symbiosis because of the appearance of the devices. People have designed, built, and studied ways of letting computers understand the operator’s hands directly, free from the limitations of intermediary devices. The commercialization and the widespread availability of devices such as Visual Programming Language (VPL) Data Glove [9] and Mattel Power Glove [10] have led to an explosion of research. Many glove-based devices (for input or output) allow the applications to be expanded into diverse fields including teleoperation, virtual reality, medicine, scientiﬁc visualization, puppetry, music, entertainment, and games [11]. Defanti and Sandin [12] developed an inexpensive and lightweight Sayre glove to monitor hand movements, which was used as an effective method for multidimensional control, not as a gesture device. Grimes of Bell Telephone Laboratories developed Digital Data Entry Glove, which is speci ally tailored to data entr y using an alphabet of hand signs [13]. Zimmerman et al. developed the Data Glove that monitored ten ﬁnger joints and the six degrees of freedom (DOFs) of the hand’s position and orientation [9].
    * Although Dexterous Handmaster (DHM) was developed as a master controller for the Utah/Massachusetts Institute of Technology (MIT) Dexterous Hand robot hand by Arthur D. Little, and Sacros is an exoskeletonlike device worn on the fingers and hand with 20 DOF for robotics, DHM has been successfully marketed as a tool for the clinical analysis of the hand function and impairment. A low-cost glove named Power Glove is developed as a controller for the Nintendo home video games [10]. Recently, an accelerometer-sensing glove (ASG) was developed by Pister et al. at University of California-Berkeley based on accelerometers from commercialoff-the-component, which can detect the human finger and translate motion into computer-interpreted signals and gestures [14]–[17]. The work is expanded to the well-known SmartDust project to form the basis of integrated massively distributed sensor networks, which are based on the cubic-millimeter mote containing an autonomous sensing and communication system. There have been several ﬁnger-typing systems proposed [18]–[22]. These systems are still inconvenient or very space limited even though they are wearable: for instance, Senseboard and Virtual Keyboard (VKB) have the best ﬁt on the desk.
    * In this paper, a new wearable input device named SCURRY, developed by the Samsung Advanced Institute of Technology (SAIT), is introduced. Based on inertial sensors (accelerometers and gyroscopes), this device can be used as a wearable keyboard, as well as a wearable mouse. The accelerometers in SCURRY are used to detect finger clicking, while the ones in ASG are used to detect ﬁnger curling, referring to the movement curling the ﬁnger perpendicular to the ground. Finger clicking in this paper is deﬁned as the ﬁnger typewriting movement without the physical touch. It allows a human operator to select a speciﬁc character, event, or operation as an input command by the human hand motion or to input the desired command (such as a character, etc.) by ﬁnger clicking only, without directly touching the object. This is the important difference between the ASG and the SCURRY: ASG uses the static aspect of accelerometer signals, while SCURRY uses the dynamic aspect of an accelerometer signal. Thus, because SCURRY has the characteristic sensing of even dynamic motion, it can be used as a device that interacts with the virtual environment, such as in games, by allowing for dynamic input or interaction. SCURRY is a glovelike input device that can be worn on a human hand.
    * It consists of a base module placed on the back of the hand and four ring-type modules (rings) worn on four fingers. The base module includes one controller and two angular-velocity sensors (gyroscopes), and the four rings include two-axis acceleration sensors (accelerometers). In addition, an algorithm composed of three sections is proposed, for exact ﬁnger-click recognition, to improve recognition performance by suppressing noises and unintentional commands. Experiments on a simple GUI testbed are conducted with ﬁve subjects to verify the effectiveness of the proposed algorithm. This paper is organized as follows. The SCURRY system is reviewed in Section II, which includes the system overview and the ﬁnger-click-recognition algorithm. The experimental results and discussion are presented in Section III, followed by the conclusion and future works in Section IV.
* An Energy Harvesting Wearable Ring Platform for Gesture Input on Surfaces

  * Abstract

    * This paper presents a remote gesture input solution for interacting indirectly with user interfaces on mobile and wearable devices. The proposed solution uses a wearable ring platform worn on a user’s index finger. The ring detects and interprets various gestures performed on any available surface, and wirelessly transmits the gestures to the remote device. The ring opportunistically harvests energy from an NFC-enabled phone for perpetual operation without explicit charging. We use a ﬁnger-tendon pressure-based solution to detect touch, and a light-weight audio based solution for detecting ﬁnger motion on a surface. The two-level energy efficient classiﬁcation algorithms identify 23 unique gestures that include tapping, swipes, scrolling, and strokes for hand written text entry. The classiﬁcation algorithms have an average accuracy of 73% with no explicit user training. Our implementation supports 10 hours of interactions on a surface at 2 Hz gesture frequency. The prototype was constructed with off-the-shelf components and has a form factor comparable to a large ring.
  * Introduction

    * Increasingly, users interact with their mobile devices on the go. Many of us listen to music on mobile devices while traveling, we constantly check our E-mails, shoppers browse through their shopping lists and do price comparisons while shopping, and people constantly access applications such as Facebook, Twitter, and Foursquare.
    * Interacting with mobile devices on-the-go requires the user to enter different gestures to scroll, zoom, flip, and enter text on UI elements. The mobile phone with its relatively large display has provided a unified and convenient platform for such interactions. However, more recent trends in wearable devices such as glasses, wrist bands, and watches have made such interactions limited and awkward due to the lack of touch real estate and the positioning of the device itself [5].
    * While mobile device interfaces continue to shrink, interfaces of remote display devices such as TVs and game consoles are becoming even more complex, requiring extensive maneuvering via simple remote controllers or requiring remote control with full keyboard-like capability [6]. For example, with a conventional remote control, a simple task such as entering text to search for a movie title becomes a monumental task leading to a poor user experience. The increasing user mobility and availability of ubiquitous computing and entertainment resources will create a strong demand for light-weight solutions that enable rich interaction with remote displays and augmented spaces.
    * In this paper we present a ring form-factor wearable device for gesture input. The user interacts with an arbitrary surface; such as a desk, chair arm rest, or the users clothing; with the ring-worn finger to enter multiple gestures. Our platform supports a rich set of gestures similar that found on a modern touch based UI. These gestures include “tap” for selection, “flip” gestures that quickly ﬂip through multiple windows or items in a list, “scroll” for scrolling through a web page or a list of items, and writing-based “text-entry”. Altogether, our platform can interpret 23 unique gesture primitives made on an arbitrary surface.
    * Some previous solutions for motion based gesturing support only a limited number of gestures such as a few patterns [13], or only text-entry under constrained conditions such as large character sizes [1] and are implemented on resource rich devices such as cell phones. Other solutions for gesturing on surfaces either required instrumented surfaces [11]; while others use heavy processing such as MFCC computations on audio, and power hungry sensors such as gyroscopes [21] which are impractical on resource limited devices such as a self-contained ring platform. Solutions that instrument the finger tip [20] are inconvenient due to the impact on the user’s touch.
    * Our goal is to develop a wearable platform, which is likely to be adopted by a wide user community, that provides always-available gesture input using gestures typically used in modern touch-based input devices. Developing such a wearable device poses several challenges:
    * The first challenge is achieving the right balance of usability and performance. A large heavy platform with ample resources would be uncomfortable to wear on a ﬁnger continuously, limiting the usability of the device; while a platform with a form factor and weight similar to a typical ring is an ideal platform for achieving the goals of wide adoption and availability.
    * However, these form factor and weight limitations severely limit the energy and computational resources available on the ring; to make matters worse, the ring needs to use an expensive wireless link to transfer information to a remote device. We use carefully designed sensing techniques, classification algorithms, and compact representation through local processing to achieve an acceptable level of performance without sacriﬁcing usability.
    * The second challenge is capturing user intent, in the form of accurately detecting the instances of the finger touching surface. For example, when we write on a whiteboard with a marker, we get the immediate visual feedback of what is being written, and the physical feedback of the marker touching the whiteboard. As a consequence of the physical feedback, we know exactly when we are writing on the surface versus when we are moving the marker to a new location.
    * Similar to writing on a whiteboard, when the user’s finger interacts with a surface, we need to capture the instances where the ﬁnger is actually touching the surface – indicating actual “writing” – versus the ﬁnger gliding in the air. While instrumenting a user’s ﬁngertip directly can solve this easily, it will affect the usability signiﬁcantly since we are obstructing the user’s natural touch and providing a platform that is too awkward to wear throughout the day. We use a combination of touch induced pressure and motion induced audio sensing to achieve accurate ﬁnger touch and motion detection to capture user intent.
    * A third challenge is to achieve perpetual operation where users do not have to take off the ring and charge it, giving the user an experience similar to wearing a conventional ring. While this is generally a challenge for an active device with a wireless link, this becomes even more challenging due to the limited battery capacity available on a ring platform.
    * We implemented a ring prototype that addresses these challenges. The ring uses energy efficient finger-tendon based touch detection and audio-based motion detection to capture user interaction instances. A light-weight multiclassiﬁer solution accurately classiﬁes 23 different gesture primitives. The ring harvests energy from an NFC-enabled phone held in a user’s hand to achieve perpetual operation of the ring without the need for explicit charging of the battery. With a 10 mAh battery, the ring can support more than 10 hours of active user interactions.
    * The remainder of the paper is organized as follows. Section 2, gives an overview of the ring platform. Section 3 describes our solutions for detecting surface interactions. Section 4, describes the design of our gesture classification solutions. Section 5 describes the ring prototype implementation. In section 6, we evaluate the gesture classiﬁcation solution and quantify the energy consumption and harvesting capabilities of the hardware platform. We present relevant related work in Section 7. Finally, we conclude with discussion and future work in Section 8.
* Decoding Surface Touch Typing from Hand-Tracking

  * Abstract

    * We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efﬁciently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contactbased text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an ofﬂine study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments.
  * Introduction

    * Text entry is an important task for communication and productivity in augmented reality and virtual reality (AR/VR). While conventional physical keyboards and touch screens can be incorporated into AR/VR input systems, added peripherals detract from mobile use cases and accessibility. Automatic speech recognition is more accessible, but may not be socially acceptable for certain environments or for private communication. Recent advances in computer vision have shown that hand pose can be accurately estimated using commodity depth, RGB or monochrome cameras. Commercial AR/VR devices such as the Oculus Quest and the Microsoft Hololens have started using hand-tracking for text entry. However, existing hand-tracking-based text entry solutions for AR/VR are relatively low throughput.
    * In this paper, we investigate the use of hand-tracking to enable typing on any flat surface at speeds comparable to typing on a physical keyboard. This has several advantages to existing approaches. Hand-tracking is typically achieved through onheadset sensors, without extra peripherals, making it suitable for on-the-go use cases. Typing on a virtual keyboard is more discreet than speaking. Most importantly, there already exists a wide audience of effective typists. Surveys of internet users show that even average typists can achieve speeds greater than 50 words per minute (WPM) with the fastest 90th percentile achieving more than 78 WPM [3]. We show that these users can transfer their existing skills typing on a keyboard to typing on a flat surface.
    * People type faster on a physical keyboard [3] than a soft keyboard and also faster on a surface [4] than in the air. In our study we compromise between accessibility and speed by requiring users to type against a surface but eschewing the use of a physical keyboard or a touchpad. Additionally, surface typing has been shown to be more comfortable than mid air typing [4] and confers haptic feedback, which enhances presence in VR [16]. Our investigation caters particularly to the fastest typists–touch typists, which we mean as those who type without the sense of sight to find the keys. Without the feel of physical keys and without using sight to ﬁnd the keys, ﬁngers will drift during typing. Instead of requiring users to make precise contacts on a ﬁxed keyboard layout, we investigate using a motion model to recognize ﬁnger trajectories, and we further explore how statistical decoding techniques affect performance.
    * We are inspired by the work of Dudley and colleagues [4] that shows the high potential of human typing efficiency and error rate given a text decoding oracle with knowledge of the text being typed. In this work, we take the ﬁrst steps to reducing this oracle to practice by building a neural model of text decoding that combines motion modeling of ﬁngers and a state-of-theart language model. Notably, our model is the ﬁrst technique to our knowledge that converts skeletal hand motion directly into text. Instead of relying on surface contact information (e.g., from capacitive touch), we investigate using the output of a marker-based hand-tracking system [12]. Hand-tracking provides potentially richer sensing information (including ﬁnger identities and trajectories) than the contact modality. While marker-based hand-tracking is still an optimistic approximation of the ﬁdelity achievable from an AR/VR headset, our experiments shed light on the potential of hand-tracking-based text decoding. Speciﬁcally, we make the following contributions:

      * We propose a motion model, represented as a temporal convolutional network (TCN), that can translate hand motion directly into (a probability distribution over the) typed text.
      * We show that we can combine our motion model with a language model, also represented as a neural network, using an efficient beam search decoding technique.
      * We show that tracking hands typing on a flat surface combined with our statistical decoding method has the potential of achieving speeds comparable to typing on a physical keyboard while maintaining low-error rates. We also explore why decoding continuous hand motion data could be advantageous to decoding a discrete set of contacts on a touch surface by isolating the value finger trajectory information, ﬁnger identity information and continuous decoding.
* Argot: A Wearable One-Handed Keyboard Glove

  * Abstract

    * The Argot glove is a one-handed, wearable input device that allows a user to type all English letters, numbers, and symbols without use of a traditional keyboard. The device design considers variables and constraints such as dexterity, feedback, mobility, learnability, speed of input, errors and false inputs, permanence, and comfort, as well as previous user knowledge. The glove design was informed by experimental investigations aimed at balancing tradeoffs between physical variables (reach, dexterity, haptics) and cognitive variables (learnability, text-entry method). It uses weak magnetic interactions during “key” presses to provide passive haptic feedback and reduce the need for precision in proprioceptive hand positioning.
  * Introduction

    * Argot addresses the pervasive challenge of enabling text input in wearable applications without fully occupying the hands. Existing approaches to wearable text input like the Twiddler chording keyboard [1] often require that a device be held and/or strapped to the hand. While this naturally prevents the hand from being used for other purposes while typing, a hand-held device can also cause add-on usability effects as the device must be retrieved prior to use, stowed following use, or held when not in use. Approaches that don’t rely on button-presses, such as gestural input devices, often require that the user learn a new vocabulary or input language.
* Airwriting: Hands-free Mobile Text Input by Spotting and Continuous Recognition of 3d-Space Handwriting with Inertial Sensors

  * Abstract

    * We present an input method which enables complex hands-free interaction through 3d handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. Motion sensing is done wirelessly by accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a Support Vector Machine to identify data segments which contain handwriting. The recognition stage uses Hidden Markov Models (HMM) to generate the text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary with over 8000 words. A statistical language model is used to enhance recognition performance and restrict the search space. We report the results from a nine-user experiment on sentence recognition for person dependent and person independent setups on 3d-space handwriting data. For the person independent setup, a word error rate of 11% is achieved, for the person dependent setup 3% are achieved. We evaluate the spotting algorithm in a second experiment on a realistic dataset including everyday activities and achieve a sample based recall of 99% and a precision of 25%. We show that additional ﬁltering in the recognition stage can detect up to 99% of the false positive segments.
  * Introduction

    * Gestures facilitate new forms of user interfaces, which will be especially suited for mobile and wearable computer systems. Instead of forcing a user to manually operate a device, hand gestures allow operation with empty hands and without the need to focus on tiny screens and keys. Various sensing techniques (e.g. cameras, inertial sensors) are used for the purpose of gesture recognition [1]. Accelerometers are especially appealing for mobile usage because of their small size, low cost and independence from the environmental conditions. Past research mostly concentrated on the recognition of a limited set of predefined single gestures, which can then be mapped to commands. This limits the number of possible commands to the number of recognizable gestures. Operations like the input of text or other complex operations require more expressive power than a small set of isolated gestures can offer.
    * Our approach combines the mobility and intuitivity of gestures as an input modality with the expressiveness of handwriting. This paper describes a wearable gesture recognition method based on inertial sensors that allows for spotting and continuously recognizing whole sentences written in the air. This comprises two main challenges. Firstly, in a real-world application scenario, the system will continuously measure hand motion, but only a small part of the signal will actually contain handwriting. The large part of the data will contain all sorts of everyday motions which are irrelevant for the text input interface. The data segments likely to contain handwriting must be spotted in the data stream. Secondly, the actual text must be recognized from the sensor signals. We use a two-stage approach for the spotting and recognition task which is illustrated in Figure 1. In the spotting stage, we use Support Vector Machines to discriminate motion that potentially contains handwriting from motion that does not. In the recognition stage, we use Hidden Markov Models in conjunction with a statistical language model to recognize the written words. The two stages are evaluated on different data sets independently but we show that combining them by applying additional filtering of false positive segments in the recognition stage can signiﬁcantly boost performance of the spotting stage.
    * While our current system is focused on the recognition of text from continuous character gestures, it can serve as a proof-of-concept system for any sort of gesture recognition system that needs to continuously recognize gestures which are constructed from an alphabet of primitive gestures. We show that continuous gesture recognition with inertial sensors is feasible for gesture vocabularies that are several orders of magnitude larger than known systems.
* Vision-Based Handwriting Recognition for Unrestricted Text Input in Mid-Air

  * Abstract

    * We propose a vision-based system that recognizes handwriting in mid-air. The system does not depend on sensors or markers attached to the users and allows unrestricted character and word input from any position. It is the result of combining handwriting recognition based on Hidden Markov Models with multi-camera 3D hand tracking. We evaluated the system for both quantitative and qualitative aspects. The system achieves recognition rates of 86.15% for character and 97.54% for small-vocabulary isolated word recognition. Limitations are due to slow and low-resolution cameras or physical strain. Overall, the proposed handwriting recognition system provides an easy-to-use and accurate text input modality without placing restrictions on the users.
  * Introduction

    * With current advances in technology, we see a rapidly increasing availability, but also demand, for intuitive humanmachine interaction. Devices are not only controlled by mouse and keyboard anymore, but we are now using gesturecontrolled devices in public areas and at our homes. Distant hand gestures in particular removed the restriction to operate a device directly; we can now interact freely with machines while moving around. In this work, we are interested in human-machine interaction that does not force users to touch a specific device or to wear special sensors, but that allows for unrestricted use.
    * While there is a great variety of human-computer interaction techniques to interact with distant virtual objects, e.g. to select menu entries, there is still a lack for intuitive and unrestricted text input. Although there are ways to input text by using a virtual keyboard on a display [11] or by speech recognition, there are situations where both are not suitable: the first requires interaction with a display and occupies space on it and for the latter, users must speak which is not always possible depending on the surroundings. With this work, we extend the available text input modalities by introducing an intuitive handwriting recognition system.
    * In the remainder of this work, we present a system that combines vision-based 3D hand tracking with 3D handwriting recognition. It allows users to write characters and words in mid-air. This extends the use of 3D hand gestures to allow for convenient and unrestricted text input. In Section 2, we discuss related work before introducing the proposed system in Section 3. In Section 4, we present the experimental evaluation before concluding in Section 5.
* Selection-Based Mid-Air Text Entry on Large Displays

  * Abstract

    * Most text entry methods require users to have physical devices within reach. In many contexts of use, such as around large displays where users need to move freely, device-dependent methods are ill suited. We explore how selection-based text entry methods may be adapted for use in mid-air. Initially, we analyze the design space for text entry in mid-air, focusing on singlecharacter input with one hand. We propose three text entry methods: H4 MidAir (an adaptation of a game controller-based method by MacKenzie et al. [21]), MultiTap (a mid-air variant of a mobile phone text entry method), and Projected QWERTY (a mid-air variant of the QWERTY keyboard). After six sessions, participants reached an average of 13.2 words per minute (WPM) with the most successful method, Projected QWERTY. Users rated this method highest on satisfaction and it resulted in the least physical movement.
  * Introduction

    * Devices and interaction techniques for text entry are much researched [24], and it is clear that the effectiveness of text entry is shaped by the context of use. For instance, mobile text entry is different from desktop text entry [22,30], and typing on a tactile keyboard requires little or no visual attention, whereas text entry on a touch surface requires visual attention. Thus, text entry in non-desktop settings presents new challenges and requires new methods [39].
    * The present paper is motivated by a need to support text entry in one such setting, users working with a large high-resolution display. Large high-resolution displays have been shown to improve productivity [11] and, in contrast to desktop displays, they promote physical movement [3]. Around large displays, users can move in order to navigate, explore, and make sense of data on the display. We seek to design text entry methods that allow users to move in front of the display, without having to hold a device or move to a fixed location to be able to enter text.
    * Recent research has helped users interact with large displays by supporting object selection and manipulation (e.g., [5,14,19,35]). Mid-air interaction [16], based on tracking of users’ hands, may work well for interaction in the context where users move in front of a large display. Vogel and Balakrishnan [35], for instance, used Vicon-tracking to let users point to a large display from a distance and manipulate the cursor; Nancel et al. [27] showed how mid-air gestures can be used to navigate a large display.
    * Whereas mid-air interactions have been explored for selection and manipulation, they are rarely used for text entry. Prior work approximates mid-air interaction by using devices such as the Nintendo Wiimote [9,33]. Other mid-air text entry techniques include AirStroke [28], a glove- and vision-based method using the Graffiti unistoke alphabet [10]. AirStroke provided a text entry rate 6.5 words per minute (WPM) without word completion. Kristensson et al. [20] demonstrated continuous recognition of mid-air gestures for writing Graffiti letters using a Kinect sensor to detect gestures within a predefined input zone.
    * We adapt existing selection-based text entry methods to mid-air interaction with large displays. Selection-based methods rely on series of movements and activations of UI components to facilitate text entry. We do so for several reasons: (1) Leveraging familiarity with existing techniques help users learn the techniques faster, which is preferable for walk-up-and-use contexts of large displays. (2) Although mid-air text entry can potentially benefit from the increased expressiveness and additional degrees of freedom of spatial 3D input, simple and effortless techniques is recommended when the user's goal is simple [7]. (3) Despite the potential of more expressive input, the most successful mid-air text entry method to date has to our knowledge been the ray-casting selection-based QWERTY method of Shoemaker et al. [33]. More studies of adaptations of text entry methods from other contexts, such as desktop or mobile computing, are needed in order to establish a base line for mid-air text entry. In order to simplify comparison, we have chosen to focus on single-character input (rather than predictive input) and on one-handed input.
    * In this paper, we contribute an analysis of the design space for mid-air text entry using a structured approach that enables researchers to relate future analyses to ours. Further, we contribute an evaluation of three mid-air text entry methods that match the context of using large high-resolution displays. The methods we propose are adapted versions of previously successful methods from three different domains; game controller text entry, mobile phone text entry, and a previously successful midair text entry method. The methods provide a solid baseline for comparison of future mid-air text entry methods.
* AirStroke: Bringing Unistroke Text Entry to Freehand Gesture Interfaces

  * Abstract

    * In this paper, we explore the opportunity of bringing unistroke text entry to freehand gesture interfaces. Using existing text entry methods directly in such interfaces is impractical because of the differences between freehand gestures and traditional forms of input. To address this problem, we consider the design constraints of text entry methods using freehand gestures, and present AirStroke, a new technique based on a reengineering of the well-known unistroke technique Graffiti. Using Graffiti’s alphabet, AirStroke takes advantage of the richer input capabilities of two-handed freehand gestures by providing combined mode selection and character entry with one hand, as well as word completion with the other hand. A longitudinal study suggests that AirStroke has competitive speed and accuracy to unistroke methods based on stylus input.
  * Introduction

    * Freehand gestures (i.e., bare hand interaction in free space or mid-air) have been expected to deliver “casual” and natural yet powerful interaction, especially in non-desktop scenarios where mice and keyboards are not appropriate or available [10, 11]. Researchers have explored the use of freehand interaction for technologies such as large displays [10], virtual reality [2], and curved surfaces [1].
    * Previous research offers freehand gesture techniques for pointing and clicking [10], window management [11], and interacting with immersive data [1]. However, research on text entry, a fundamental interaction task in many applications, using freehand gesture input is rare.
    * Since numerous text input methods have been developed for other types of interfaces, it appears straightforward at first glance to adapt existing methods to freehand gesture input. Most of them, however, were developed for very different input devices such as keyboard, mouse or stylus. It is not certain if they are applicable to freehand text entry, since freehand gesture input is different from traditional input devices in several respects:

      * There is no explicit “button” to trigger discrete events, and similarly, there is no explicit out-of-range delimiter, which is important for segmenting continuous gestures [7].
      * Freehand pointing precision is lower than that pointing with a mouse or gesturing on a stable surface with a pen [10], and varies with the distance between the user and the display [9, 10]. Since freehand interactions involve relatively unconstrained user movement, distance-independent [9] techniques are preferable.
      * Freehand gestures involve more muscles than keyboard or stylus interaction and thus can cause more fatigue [2]. Therefore, freehand interactions should avoid gestures that require high precision over a long period of time [2].
    * In this paper, we explore the opportunity of bringing unistroke text entry to freehand gesture interfaces. To do so, we first examine existing methods from the literature, and discuss the difficulties of using them with freehand gestures. Given the constraints, we present our design and implementation of a freehand gesture text entry method called AirStroke, and report on a longitudinal study of its speed and accuracy.
    * AirStroke is built upon Graffiti [3], a well-known unistroke method, but it is more than simply using Graffiti in mid-air. Rather, AirStroke takes advantage of the richer input capabilities of two-handed freehand gestures by providing combined mode selection and unistroke character entry with one hand, as well as optional word prediction and completion with the other hand.
* WrisText: One-handed Text Entry on Smartwatch using Wrist Gestures

  * Abstract

    * We present WrisText - a one-handed text entry technique for smartwatches using the joystick-like motion of the wrist. A user enters text by whirling the wrist of the watch hand, towards six directions which each represent a key in a circular keyboard, and where the letters are distributed in an alphabetical order. The design of WrisText was an iterative process, where we first conducted a study to investigate optimal key size, and found that keys needed to be 55º or wider to achieve over 90% striking accuracy. We then computed an optimal keyboard layout, considering a joint optimization problem of striking accuracy, striking comfort, word disambiguation. We evaluated the performance of WrisText through a five-day study with 10 participants in two text entry scenarios: hand-up and handdown. On average, participants achieved a text entry speed of 9.9 WPM across all sessions, and were able to type as fast as 15.2 WPM by the end of the last day.
  * Introduction

    * Text entry is a common and important task in daily mobile life [7], comprising of approximately 40% of mobile activity [10]. However, entering text on a smartwatch is challenging because of the small form factor and its wearable context. One of the most commonly observed problems is the need to use one or both hands for a task (e.g. driving or walking while holding an umbrella or shopping bags). This is cumbersome in the context of smartwatches, as a user is required to interrupt their ongoing task to enter text, which reduces the purposefulness of smartwatches, as they are predominantly valuable for accessing information while on-the-go.
    * To mitigate this problem, one solution is speech input, which is socially inappropriate in some situations (e.g., at meetings or classrooms) [61], and may also expose the users’ privacy. Another solution is to enable one-handed interaction for smartwatches using the same-side hand (SSH) [31]. However, prior work has primarily been targeted at general interactions, such as assigning discrete commands to micro-interactions [36, 58], finger postures [16, 46, 63], continuous gestural input [19, 51]. Onehanded text entry has been largely overlooked.
    * In this paper, we present WrisText, a one-handed text entry technique for smartwatches using the wrist’s joystick-like motion [19] (Figure 1). With it, a user whirls the wrist of the same-side hand to strike directional marks to select keys on a circular keyboard on a smartwatch. To explore the design space of this new text entry technique, we took an iterative design approach, where we optimized the keyboard layout based on a number of factors, including keyboard learnability, striking accuracy, word disambiguation, and striking comfort. We first conducted a target acquisition task to determine the proper size of the arc-shaped keyboard keys (e.g., 55.4˚). Based on the result, we designed a keyboard layout with six keys, containing groups of four to five English letters following an alphabetical order (Figure 1). Next, we performed a stepwise search for optimal layout variations, and identify one that balances striking accuracy, striking comfort, and word disambiguation (Figure 1). Finally, we conducted a 5-day study with ten participants to evaluate the speed and accuracy of WrisText in common smartwatch usage scenarios, such as holding the smartwatch in front of the chest and placing the hand downwards, alongside the body. Our results revealed that participants could achieve an average of 15.2 (s.e. = 0.5) WPM in the fifth day with 0.1% uncorrected errors. Extending the study by three more days with two randomly picked participants improved the speed further to 24.9 WPM.
    * Our contributions for this work include: (1) a one-handed text entry technique on smartwatches using the wrist’s joystick motion; (2) an optimized keyboard layout design for WrisText; and (3) a demonstration of the effectiveness of WrisText through a 5-day user study.
* Vulture: A Mid-Air Word-Gesture Keyboard

  * Abstract

    * Word-gesture keyboards enable fast text entry by letting users draw the shape of a word on the input surface. Such keyboards have been used extensively for touch devices, but not in mid-air, even though their fluent gestural input seems well suited for this modality. We present Vulture, a word-gesture keyboard for mid-air operation. Vulture adapts touch based word-gesture algorithms to work in midair, projects users’ movement onto the display, and uses pinch as a word delimiter. A first 10-session study suggests text-entry rates of 20.6 Words Per Minute (WPM) and finds hand-movement speed to be the primary predictor of WPM. A second study shows that with training on a few phrases, participants do 28.1 WPM, 59% of the text-entry rate of direct touch input. Participants’ recall of trained gestures in mid-air was low, suggesting that visual feedback is important but also limits performance. Based on data from the studies, we discuss improvements to Vulture and some alternative designs for mid-air text entry.
  * Introduction

    * Mid-air interaction is an emerging input modality for large displays [20], mobile phones [9], augmented reality [23], and desktop computers [29]. Facilitated by improved tracking equipment, mid-air techniques cover many types of interaction. For instance, mid-air pointing enables selection and manipulation of objects (e.g., [2, 6, 10, 28, 30]). Writing text in mid-air, however, has received less attention. Text entry is an important activity and supporting it in mid-air would be beneficial for a number of scenarios such as work in sterile conditions (e.g., operating theatres), in augmented reality (e.g., with Google Glass), and when writing on public displays. It has been shown that people can write in mid-air with devices such as game controllers and dedicated gloves [21], but also using their hands [12, 18]. However, text-entry rates for mid-air interaction are low, around 13 [18] to 18.9 WPM [27]; the latter rate was obtained with tactile feedback on errors and no character production on errors. Furthermore, most techniques support only single-character text entry (e.g., [18, 21, 27]). So midair text entry is still relatively slow and better techniques should be developed to make mid-air text entry practical. Hence, we study if speed can be improved by moving from selection-based text entry to gestural input.
    * Word-gesture keyboards (WGK) (e.g., SlideIT, Swype and ShapeWriter) have gained popularity (e.g., a WGK is now shipped as standard on Android devices) and perform well on touch screens [11, 36, 37]. The key idea of WGKs is that the user enters a word by drawing the pattern formed by its letters on the input surface rather than by typing the letters. When implemented with the QWERTY layout, WGKs allow users to benefit from previous experience. Furthermore, WGKs provide a fluent way of writing words as gestures while also supporting simple tapping input.
    * The present paper suggests that WGKs may be beneficial to mid-air text entry. However, transferring WGK to mid-air is hard: what are the delimiters of words, what is the equivalent of tapping and releasing in mid-air, and will the prediction algorithms for WGKs, developed for direct surface input [13], work in mid-air?
    * In the rest of the paper we describe a system, Vulture, for doing mid-air text entry that answers some of these questions. We also describe two formative studies of Vulture: one study estimates the text-entry rate of Vulture and another study compares the performance and recall of gestures with Vulture to a touch-based WGK.
* Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion

  * Abstract

    * We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users’ intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user’s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1%. This performance represents a 19.6% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.
  * Introduction

    * Recent progress in head-mounted displays (HMDs) for augmented reality (AR), such as the Microsoft HoloLens, demonstrates the commercial potential of AR to support new forms of interaction and work in a range of industries including construction, education and health. Text entry is an integral activity in such AR environments, allowing users to, for example, send short messages, annotate physical objects and digital content, compose documents or fill out forms. The placement of the user within a virtually augmented environment introduces new and exciting opportunities for the interface designer. The design space is considerably broadened by the additional dimensionality available and new forms of interaction are made possible.
    * Newchallenges also emerge in providing effective text entry for ARHMDs. First, currently available devices are typically limited in terms of their display region size. Compounding the limited size is the fact that the display region is located in the centre of the user’s field-of-view. Delivering a text entry method that preserves field-of-view while supporting effective input presents a unique design challenge. Second, delivering immersive and fully mobile AR applications in which the user can freely explore and interact with both the physical and virtual environment suggests avoiding input devices that encumber the user. Avoiding encumbering the user while maintaining freedom ofmobility means that external (off-body) sensing to support text entry is also not practical. Third, a key goal ofAR applications in general should be tominimise or eliminate the distinction between physical and virtual content from the perspective of the user. A text entry method for AR should thus be consistent with the broader experience and maintain any developed sense of immersion.
    * In response to the identified challenges, this article presents a novel system that enables users to type on a virtual keyboard using a head-mounted AR device and hand localisation derived from body-fixed sensors.We call this system the virtualised input surface for augmented reality (VISAR) keyboard. The VISAR keyboard is a probabilistic auto-correcting translucent keyboard system with variable occlusion, specifically designed for ARHMDs, such as theMicrosoft HoloLens.Our system seeks to leverage learned keyboard interaction behaviour and exploit the additional dimensionality of the design space available in AR. By adapting a state-of-the-art probabilistic decoder, we enable people to type in a fashion that is familiar and akin to typing on their mobile phone keyboard or on a wall-mounted touch-capable display. To the authors’ knowledge, this is the first investigation of providing a touch-driven text entry method specifically designed for AR and based upon bodyfixed (as opposed to space-fixed) sensor data. The system is thus fully encapsulated by the headmounted device and enables truly mobile, unencumbered text entry for AR. Furthermore, we seek to specifically address the unique design requirements ofoptical see-through head-mounted AR by accommodating design objectives that relate to the constrained display size, and therefore explore minimising occlusion of the user’s field-of-view. We also guide future text entry design for AR HMDs by presenting the results of four experiments and one validation study that investigate the implications of the design choices in the VISAR keyboard. The design of VISAR is underpinned by six solution principles for AR text entry, which we have distilled from the literature and prior experience in text entry design.
    * Experiment 1 revealed that novice users with minimal practice reach entry rates that are comparable with the current standard interaction technique used within the Microsoft HoloLens default system keyboard, which requires that users move their head to place a gaze-directed cursor on the desired key, and then perform a hand gesture. However, we find that in terms ofdiscrete selection events, the virtual touch technique used in VISAR is on average 17.4% faster than the baseline method.
    * In Experiment 2, we investigated how allowing users to seamlessly shift from probabilistic autocorrecting text entry to literal text input without an explicit mode-switch affects performance. The results revealed that users most commonly exploited the inferred-to-literal fall-back method to pre-emptively enter words they did not expect the decoder to recognise. The inferred-to-literal fall-back method introduces a speed penalty due to the requirement to dwell on a key to make a selection. Despite this penalty associated with dwell, for phrases with a high degree ofuncertainty under the language model, participants were able to type as quickly as they did without the fallback method but with reduced character error rates (CER).
    * Experiment 3 revealed that the interaction techniques applied in VISAR enable users to type effectively even when key outlines and labels are hidden. Out ofthe 12 participants who completed both Experiment 2 and 3, 10 achieved their highest entry rates in one of the two reduced occlusion configurations examined. This shows that the majority ofparticipants were able to readily transfer their learned typing skills to the novel interface approach. Varying keyboard occlusion in AR typing has to our knowledge never been proposed or explored before.
    * Experiment 4 returns to the baseline comparison but with the design improvements identified in Experiments 1 to 3 incorporated into VISAR and with the addition of word predictions. User performance was evaluated under extended use with participants typing between 1.5 to 2 hours in each condition over a fixed number of test blocks. We demonstrate that the refined VISAR design achieves a mean entry rate of 16.76 words-per-minute (wpm) compared with 14.26wpm in the baseline condition. Analysing only the results from the final four blocks in each condition (i.e., after the most pronounced learning effect has subsided), the mean entry rates are then 17.75wpm and 14.84wpm for the VISAR and the baseline conditions, respectively.
    * Finally, we conducted a validation study, which demonstrated that the VISAR keyboard is a viable text entry method for typical text entry tasks anticipated for productive use ofAR HMDs. The user experience of the system is examined in four sub-tasks involving transcription, composition, replying to a message, and freely annotating real-world objects.

‍
